{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn_09_mnist_ensemble.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMqHk9rAoXfEYBdQleSc65T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lhs7091/PythonTensorExam/blob/master/exam_11/cnn_09_mnist_ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cty6w3Hvu3Eq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bf43d889-2c2d-4a8f-89b2-d034519b239e"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "tf.set_random_seed(777)\n",
        "\n",
        "# check out information of mnist data\n",
        "mnist = input_data.read_data_sets(\"./mist_data\", one_hot=True)\n",
        "\n",
        "# parameters\n",
        "learning_rate = 0.001\n",
        "training_epochs = 20\n",
        "batch_size = 100\n",
        "nb_classes = 10\n",
        "\n",
        "\n",
        "class Model:\n",
        "    def __init__(self, sess, name):\n",
        "        self.sess = sess\n",
        "        self.name = name\n",
        "        self._build_net()\n",
        "\n",
        "    def _build_net(self):\n",
        "        with tf.variable_scope(self.name):\n",
        "            # dropout(keep_prob) rate is set 0.5 ~ 0.7\n",
        "            # but on training, should be set 1\n",
        "            self.training = tf.placeholder(tf.bool)\n",
        "\n",
        "            # input place holders\n",
        "            self.x = tf.placeholder(tf.float32, [None, 784])\n",
        "            x_image = tf.reshape(self.x, [-1, 28, 28, 1])\n",
        "            self.y = tf.placeholder(tf.float32, [None, nb_classes])\n",
        "\n",
        "            # Convolution Layer 1\n",
        "            conv1 = tf.layers.conv2d(inputs=x_image, filters=32, kernel_size=[3,3], padding='SAME', activation=tf.nn.relu)\n",
        "            # Pooling Layer 1\n",
        "            pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2,2], strides=2, padding='SAME')\n",
        "            dropout1 = tf.layers.dropout(inputs=pool1, rate=0.3, training=self.training)\n",
        "\n",
        "            # Convolution Layer 2\n",
        "            conv2 = tf.layers.conv2d(inputs=dropout1, filters=64, kernel_size=[3,3], padding='SAME', activation=tf.nn.relu)\n",
        "            # Pooling Layer 2\n",
        "            pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2,2], strides=2, padding='SAME')\n",
        "            dropout2 = tf.layers.dropout(inputs=pool2, rate=0.3, training=self.training)\n",
        "\n",
        "            # Convolution Layer 3\n",
        "            conv3 = tf.layers.conv2d(inputs=dropout2, filters=128, kernel_size=[3,3], padding='SAME', activation=tf.nn.relu)\n",
        "            # Pooling Layer 3\n",
        "            pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2,2], strides=2, padding='SAME')\n",
        "            dropout3 = tf.layers.dropout(inputs=pool3, rate=0.3, training=self.training)\n",
        "\n",
        "            # Fully Connected(Dense) Layer with Relu\n",
        "            \"\"\"\n",
        "            Densely-connected layer class.\n",
        "            This layer implements the operation: outputs = activation(inputs * kernel + bias) \n",
        "            Where activation is the activation function passed as the activation argument (if not None), \n",
        "            kernel is a weights matrix created by the layer, \n",
        "            and bias is a bias vector created by the layer (only if use_bias is True).\n",
        "            \"\"\"\n",
        "            flat = tf.reshape(dropout3, [-1,4*4*128])\n",
        "            dense4 = tf.layers.dense(inputs=flat, units=625, activation=tf.nn.relu)\n",
        "            dropout4 = tf.layers.dropout(inputs=dense4, rate=0.5, training=self.training)\n",
        "\n",
        "            # Logits(no activation) Layer: L5 final FC 625 inputs -> 10 outputs\n",
        "            self.logits = tf.layers.dense(inputs=dropout4, units=10)\n",
        "\n",
        "        # define cost/loss & optimizer\n",
        "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.y))\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(self.cost)\n",
        "\n",
        "        correct_prediction = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.y, 1))\n",
        "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "    def predict(self, x_test, training=False):\n",
        "        return self.sess.run(self.logits, feed_dict={self.x:x_test, self.training:training})\n",
        "\n",
        "    def get_accuracy(self, x_test, y_test, training=False):\n",
        "        return self.sess.run(self.accuracy, feed_dict={self.x:x_test, self.y:y_test, self.training:training})\n",
        "\n",
        "    def train(self, x_test, y_test, training=True):\n",
        "        return self.sess.run([self.cost, self.optimizer], feed_dict={self.x:x_test, self.y:y_test, self.training:training})\n",
        "\n",
        "\n",
        "# initialize\n",
        "sess = tf.Session()\n",
        "\n",
        "models = []\n",
        "num_model = 2\n",
        "for m in range(num_model):\n",
        "    models.append(Model(sess, \"model\"+str(m)))\n",
        "\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "print('Learning Start')\n",
        "\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost_list = np.zeros(len(models)) #a new array of given shape and type, filled with zeros.\n",
        "    total_batch = int(mnist.train.num_examples / batch_size)\n",
        "    for i in range(total_batch):\n",
        "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "\n",
        "        # train each model\n",
        "        for m_idx, m in enumerate(models): #enumerate(iterable[, start]) -> iterator for index, value of iterable\n",
        "            c, _ = m.train(batch_xs, batch_ys)\n",
        "            avg_cost_list[m_idx] += c/total_batch\n",
        "\n",
        "    print('Epoch:', '%04d'%(epoch+1), 'cost:', avg_cost_list)\n",
        "\n",
        "print('Learning Finish')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-1-4adc87a18951>:10: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting ./mist_data/train-images-idx3-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting ./mist_data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Extracting ./mist_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting ./mist_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From <ipython-input-1-4adc87a18951>:37: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2D` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From <ipython-input-1-4adc87a18951>:39: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.MaxPooling2D instead.\n",
            "WARNING:tensorflow:From <ipython-input-1-4adc87a18951>:40: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "WARNING:tensorflow:From <ipython-input-1-4adc87a18951>:63: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From <ipython-input-1-4adc87a18951>:70: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "Learning Start\n",
            "Epoch: 0001 cost: [0.2829559  0.29450094]\n",
            "Epoch: 0002 cost: [0.08598883 0.09015049]\n",
            "Epoch: 0003 cost: [0.06787695 0.0671371 ]\n",
            "Epoch: 0004 cost: [0.05525046 0.05601343]\n",
            "Epoch: 0005 cost: [0.0483394  0.05198433]\n",
            "Epoch: 0006 cost: [0.04458692 0.04589637]\n",
            "Epoch: 0007 cost: [0.03965247 0.04378542]\n",
            "Epoch: 0008 cost: [0.03931586 0.03901788]\n",
            "Epoch: 0009 cost: [0.0356669  0.03851119]\n",
            "Epoch: 0010 cost: [0.03298675 0.03421564]\n",
            "Epoch: 0011 cost: [0.03394113 0.03465421]\n",
            "Epoch: 0012 cost: [0.03087404 0.03118335]\n",
            "Epoch: 0013 cost: [0.02846079 0.02911809]\n",
            "Epoch: 0014 cost: [0.02794101 0.02726312]\n",
            "Epoch: 0015 cost: [0.02596929 0.02866644]\n",
            "Epoch: 0016 cost: [0.02504249 0.02644432]\n",
            "Epoch: 0017 cost: [0.02369641 0.02578337]\n",
            "Epoch: 0018 cost: [0.02370993 0.02550837]\n",
            "Epoch: 0019 cost: [0.02327569 0.02570278]\n",
            "Epoch: 0020 cost: [0.02202961 0.02173401]\n",
            "Learning Finish\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrsgLssiy_b-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "11f24eed-1b73-47f3-b83e-84e5f28abd13"
      },
      "source": [
        "# Test model and check accuracy\n",
        "test_size = len(mnist.test.labels)\n",
        "predictions = np.zeros([test_size, 10])\n",
        "for m_idx, m in enumerate(models):\n",
        "    print((m_idx+1), 'Accuracy:', m.get_accuracy(mnist.test.images, mnist.test.labels))\n",
        "    p = m.predict(mnist.test.images)\n",
        "    predictions += p\n",
        "    \n",
        "ensemble_correct_prediction = tf.equal(tf.argmax(predictions, 1), tf.argmax(mnist.test.labels, 1))\n",
        "ensemble_accuracy = tf.reduce_mean(tf.cast(ensemble_correct_prediction, tf.float32))\n",
        "print('Ensemble accuracy:', sess.run(ensemble_accuracy))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 Accuracy: 0.994\n",
            "2 Accuracy: 0.9939\n",
            "Ensemble accuracy: 0.9948\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}